{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba59169",
   "metadata": {},
   "source": [
    "# **Group 14 Apply AI Project: _________________________________**                                   \n",
    "Members: Torrence Barbour, Satwik Nijampudi, Shanthan Gunti, and Ryan Edwards\n",
    "\n",
    "Given a set of posts found on Reddit marked with if the user had depression or not, can we train a model to detect depression in a social media user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd3c8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to import some libraries to do this, including some big ones like numpy, pandas, and matplotlib\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546ff38",
   "metadata": {},
   "source": [
    "## Further Cleaning our data                                                                                                  \n",
    "We got our data from a kaggle dataset: https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned and it came pre-cleaned, however we want to further clean this dataset to suit our intended purposes (like lemmatization, removing filler words, and more). We will do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eaa3243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i ve kind of stuffed around a lot in my life d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sleep is my greatest and most comforting escap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  is_depression\n",
       "0  we understand that most people who reply immed...              1\n",
       "1  welcome to r depression s check in post a plac...              1\n",
       "2  anyone else instead of sleeping more when depr...              1\n",
       "3  i ve kind of stuffed around a lot in my life d...              1\n",
       "4  sleep is my greatest and most comforting escap...              1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we will need to bring in our dataset. We can use numpy for this\n",
    "text = pd.read_csv('depression_dataset_reddit_cleaned.csv')\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7245b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/lizadking/.local/lib/python3.10/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: jinja2 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (59.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.25.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/lizadking/.local/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/lizadking/.local/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/lizadking/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/lizadking/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/lizadking/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lizadking/.local/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#Next, we are going to need a tool to tokenize, lemmatize, and manage our data. We will use spacy\n",
    "import spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6a56de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_text       anyone else instead of sleeping more when depr...\n",
      "is_depression                                                    1\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Just to confirm, lets print out one of the posts\n",
    "print(text.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f2841",
   "metadata": {},
   "source": [
    "Now that we have some tools, our dataset, and another tool used for NLP installed, we need to start preparing our data so it can be used. The first step in this is something called tokenization. Essentially, each word (or even punctuation for that matter) in a sentence plays a role and it can be looked at as its own entity. We want to break each of these sentences apart into 'tokens' so they are more processable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62b4847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyone\n",
      "else\n",
      "instead\n",
      "of\n",
      "sleeping\n",
      "more\n",
      "when\n",
      "depressed\n",
      "stay\n",
      "up\n",
      "all\n",
      "night\n",
      "to\n",
      "avoid\n",
      "the\n",
      "next\n",
      "day\n",
      "from\n",
      "coming\n",
      "sooner\n",
      "may\n",
      "be\n",
      "the\n",
      "social\n",
      "anxiety\n",
      "in\n",
      "me\n",
      "but\n",
      "life\n",
      "is\n",
      "so\n",
      "much\n",
      "more\n",
      "peaceful\n",
      "when\n",
      "everyone\n",
      "else\n",
      "is\n",
      "asleep\n",
      "and\n",
      "not\n",
      "expecting\n",
      "thing\n",
      "of\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "#Lets tokenize the sample sentence from before as an example\n",
    "sample_Tokens = nlp(text.iloc[2]['clean_text'])\n",
    "for part in sample_Tokens: \n",
    "    print(part.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f24bd9",
   "metadata": {},
   "source": [
    "On top of tokenization, there is also lemmatization. A lemma in linguistics is essentially the core component of a word, before conjugation and the like occurs. This is very important because it essentially determines what role the word will play in the sentence. Will it be a noun? A verb? Lets see a few examples of this in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b387bb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemma of coming is come\n",
      "The lemma of sleeping is sleep\n",
      "The lemma of sooner is soon\n"
     ]
    }
   ],
   "source": [
    "#The .lemma_ command in spacy will output the lemma of the word it was called with. This will come in handy\n",
    "print('The lemma of', sample_Tokens[18], 'is', sample_Tokens[18].lemma_)\n",
    "print('The lemma of', sample_Tokens[4], 'is', sample_Tokens[4].lemma_)\n",
    "print('The lemma of', sample_Tokens[19], 'is', sample_Tokens[19].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef281f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The part of speech of the word come is VERB\n",
      "The part of speech of the word soon is ADV\n",
      "The part of speech of the word anxiety is NOUN\n"
     ]
    }
   ],
   "source": [
    "#The .pos_ command will output the part of speech the word plays in a sentence.\n",
    "print('The part of speech of the word', sample_Tokens[18].lemma_, 'is', sample_Tokens[18].pos_)\n",
    "print('The part of speech of the word', sample_Tokens[19].lemma_, 'is', sample_Tokens[19].pos_)\n",
    "print('The part of speech of the word', sample_Tokens[24].lemma_, 'is', sample_Tokens[24].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf67ab0",
   "metadata": {},
   "source": [
    "Now, lets use this information in order to get our dataset cleaned to the utmost. The only things left should be easily recognizable by future tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d71c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty array to store the cleaned posts in\n",
    "cleaned_Posts = []\n",
    "\n",
    "#Parse through every post\n",
    "for uncleaned_Post in range(len(text)):\n",
    "    to_Clean = text.iloc[uncleaned_Post].copy() #We need to make a copy or else we will mess with the original data\n",
    "    \n",
    "    #Lets make all the tokens lowercase so capitalization is no longer a concern (We can use PCRE regular expressions)\n",
    "    to_Clean_More = re.sub(\"[^A-Za-z']+\", ' ', to_Clean['clean_text'].replace('', ' ')).lower()\n",
    "    \n",
    "    #Tokenize and Lemmatize the posts now\n",
    "    to_Clean_Even_More = nlp(to_Clean_More)\n",
    "    absurdly_Clean_Now = [word.lemma_ for word in to_Clean_Even_More if ((not word.is_stop) or (' ' in word.text))]\n",
    "    \n",
    "    #If there is any text left in absurdly_Clean_Now the add it to the new column\n",
    "    if len(absurdly_Clean_Now) > 1:\n",
    "        to_Clean['really_Clean'] = ' '.join(absurdly_Clean_Now)\n",
    "    \n",
    "    #Now we just append the row to our cleaned_Posts array\n",
    "    cleaned_Posts.append(to_Clean)\n",
    "    \n",
    "#Create a new data frame with the new column in it\n",
    "clean_Text = pd.DataFrame(cleaned_Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "303e7c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_text       we understand that most people who reply immed...\n",
      "is_depression                                                    1\n",
      "really_Clean       w e u n d e r s t n d t h t m o s t p e o p ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#If we save the dataset now we can save a lot of time later, so lets do that\n",
    "clean_Text.to_csv('cleaned_depression_dataset.csv')\n",
    "clean_Text.head()\n",
    "print(clean_Text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9cefe0-ee87-45f7-b4bc-331af2ac293a",
   "metadata": {},
   "source": [
    "# Prepare for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7fa0ef4-7a5e-4faf-afe6-57ab7c2814c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the words to get an idea of word frequency\n",
    "from collections import Counter \n",
    "\n",
    "reviews = [review.split(' ') for review in list(clean_Text['clean_text'])]\n",
    "word_freq = Counter([token for review in reviews for token in review]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6ea9c8d-0c16-4f3b-bcb6-ed986eaec644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 40409),\n",
       " ('to', 17965),\n",
       " ('and', 16326),\n",
       " ('a', 12636),\n",
       " ('the', 11932),\n",
       " ('my', 11430),\n",
       " ('it', 9976),\n",
       " ('of', 7738),\n",
       " ('t', 7695),\n",
       " ('me', 6941)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's run a check\n",
    "word_freq[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40c464b9-c203-4a9c-bfa3-a28d52feee40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('skooool', 1),\n",
       " ('bradie', 1),\n",
       " ('vegies', 1),\n",
       " ('vickybeeching', 1),\n",
       " ('warranty', 1),\n",
       " ('voided', 1),\n",
       " ('ooooooooooooh', 1),\n",
       " ('headddd', 1),\n",
       " ('shedfire', 1),\n",
       " ('mrsshedfire', 1),\n",
       " ('bleeeech', 1),\n",
       " ('cuprohastes', 1),\n",
       " ('advert', 1),\n",
       " ('misleading', 1),\n",
       " ('civ', 1),\n",
       " ('andn', 1),\n",
       " ('ophelia', 1),\n",
       " ('sorreh', 1),\n",
       " ('spek', 1),\n",
       " ('normalz', 1),\n",
       " ('moulin', 1),\n",
       " ('rouge', 1),\n",
       " ('pirro', 1),\n",
       " ('swatching', 1),\n",
       " ('cardi', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words that were only seen once \n",
    "word_freq[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d7408f3-bbc6-48bb-a572-9bf74c3ae7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4038"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words that appear infrequently\n",
    "word_freq = dict(word_freq)\n",
    "print(len(word_freq))\n",
    "min_freq = 5\n",
    "word_dict = {}\n",
    "\n",
    "# sending all the unknowns to 0\n",
    "i = 1\n",
    "for word in word_freq:\n",
    "    if word_freq[word] > min_freq:\n",
    "        word_dict[word] = i\n",
    "        i += 1\n",
    "    else:\n",
    "        word_dict[word] = 0\n",
    "\n",
    "# dictionary length        \n",
    "dict_length = max(word_dict.values()) + 1\n",
    "dict_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51c439c-0f34-474e-b3c9-6285d494c94d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40db7e2c30fe49e8899b57b33eb40125",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4239\n"
     ]
    }
   ],
   "source": [
    "#in order to collect the tensors into batches sentences must be of the same size, it's easier to create a max sentence size and pad as nedded \n",
    "max_length = 0\n",
    "for idx in tqdm(range(len(clean_Text))):\n",
    "    row = clean_Text.iloc[idx]\n",
    "    length = len(row['clean_text'].split(' '))\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "print(max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d43b486-ac6a-4e06-96b1-ef1e83b74583",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mlong(), y\n\u001b[1;32m     26\u001b[0m ds \u001b[38;5;241m=\u001b[39m DepressionDataSet(clean_Text, word_dict, max_length)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(ds))\n",
      "Cell \u001b[0;32mIn [23], line 22\u001b[0m, in \u001b[0;36mDepressionDataSet.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(review)):\n\u001b[1;32m     18\u001b[0m     \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# we want to front pad for RNN\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     x[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(review) \u001b[38;5;241m+\u001b[39m idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_dict[review[idx]]\n\u001b[0;32m---> 22\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msent_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mis_depression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# embedding likes long tensors\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mlong(), y\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DepressionDataSet(Dataset):\n",
    "    def __init__(self, df, word_dict, max_length):\n",
    "        self.df = df\n",
    "        self.word_dict = word_dict\n",
    "        self.sent_dict = {'0': 0, '1': 1}\n",
    "        self.max_len = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        review = row['clean_text'].split(' ')\n",
    "        x = torch.zeros(self.max_len)\n",
    "        \n",
    "        # get review as a list of integers\n",
    "        for idx in range(len(review)):\n",
    "            \n",
    "            # we want to front pad for RNN\n",
    "            x[self.max_len - len(review) + idx] = self.word_dict[review[idx]]\n",
    "            \n",
    "        y = torch.tensor(self.sent_dict[row['is_depression']]).float()\n",
    "        \n",
    "        # embedding likes long tensors\n",
    "        return x.long(), y\n",
    "ds = DepressionDataSet(clean_Text, word_dict, max_length)\n",
    "next(iter(ds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742953d2-1aa4-4796-a4eb-84676967a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
